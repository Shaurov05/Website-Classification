{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"LrZTwnkam1-u","outputId":"ea744b5c-4eda-477e-bd7b-ea7d0b996ba9"},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # daimport pandas as pd\nimport numpy as np\nfrom sklearn import linear_model\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\n%matplotlib inline \nfrom sklearn.utils import shuffle\nimport matplotlib.pyplot as plt\nfrom numpy import array\nfrom numpy import argmax\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_selection import chi2\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Activation\nfrom keras.utils import np_utils\nimport re\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import one_hot\nfrom keras.preprocessing.text import text_to_word_sequence\nfrom sklearn.svm import LinearSVC\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"VLt0Y1BGm1-0"},"cell_type":"code","source":"names=['URL','Category']\ndf=pd.read_csv( \"../input/website-classification-using-url/URL Classification.csv\", names = names, na_filter = False)\n#df=pd.read_csv('../input/Website classification using URL/URL Classification.csv')\n#df=pd.read_csv('/content/drive/My Drive/Colab Notebooks shuvo/URL Classification.csv',names=names, na_filter=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"Qo6Fb29Om1-4","outputId":"b63a1f04-6b3b-4fe1-8985-ca0232c76bcd"},"cell_type":"code","source":"from nltk.stem import PorterStemmer\nimport re\ndf2 = df[:]\ndef perform_stemming(websites):\n    urls = list()\n    stemmer = PorterStemmer()\n    for website in websites:\n            rows = re.sub('[^a-zA-Z]', ' ', website)\n            row = rows.split()\n            #row = [stemmer.stem(word) for word in row]\n            row = ' '.join(row)\n            urls.append(row)\n    return urls\n\nurls = perform_stemming(websites = df['URL'])\n    \nurls[:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"vDueTkjym1--","outputId":"2e9365dc-bc4e-4771-85f3-5707d13acee5"},"cell_type":"code","source":"print('this is URL\\n',df['URL'][1004:1008])\nprint('-'*30)\nprint('this is processed urls\\n', urls[1004:1008])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"F9SlPHKAm1_D","outputId":"9977619b-4309-4bbf-bd48-b3d9e3a33435"},"cell_type":"code","source":"backup_urls = urls[:]\nurls = [row.split() for row in urls]\nmax_length = 0\ncount=0\nindex=0\nfor row in urls:\n    length = len(row)\n    count+=1\n    if max_length < length :\n        max_length = len(row)\n        text = row\n        index = count\nprint(\"Maximum length:\",max_length)\nprint(\"text:\", text)\nprint(index)\nurls[index-1:index]\ndf['URL'][index-1 : index].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"_jOJ3vEzm1_H","outputId":"07a17314-6e26-4d6f-cb77-89cd58980376"},"cell_type":"code","source":"output = list()\nprocessed_dataset = list()\ndef processed_urls(website):\n    for url in website:\n        if len(url)>20:\n            url_data = url[:20]\n        else:\n            url_data = url\n            \n        word = ' '.join(url_data)\n        output.append(word)\n        \n    return output\n    \nprocessed_dataset = processed_urls(website = urls)\n\n#finding maximun length of the website\nbackup_dataset = processed_dataset[:]\nurls = [row.split() for row in backup_dataset]\nmax_length = 0\nfor row in urls:\n    length = len(row)\n    if max_length < length :\n        max_length = len(row)\n        text = row\nprint(\"Maximum length:\",max_length)\n#print(\"text:\", text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"6QDzoNNom1_O","outputId":"cab5f515-e288-412d-de13-39be52de5681"},"cell_type":"code","source":"processed_dataset = pd.DataFrame(list(zip(processed_dataset, df['Category'])),\n                                 columns = ['URL', 'Category'])\nprocessed_dataset[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"mDmABy3Gm1_R","outputId":"e7e95f20-54df-426b-f7ce-c76e75c1bc72"},"cell_type":"code","source":"print('original url\\n' , df[495423 : 495424].values)\nprint('processed url\\n',processed_dataset[495423 : 495424].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"kKGdoP9Rm1_V","outputId":"3665283c-5314-4bcf-8752-d57d6094b3b3"},"cell_type":"code","source":"dataset = processed_dataset[:]\nadult = dataset[0:2000]\narts = dataset[50000:52000]\nbusiness = dataset[520000:522000]\ncomputers = dataset[535300:537300]\ngames = dataset[650000:652000]\nhealth = dataset[710000:712000]\nhome =  dataset[764200:766200]\nkids =  dataset[793080:795080]\nnews =  dataset[839730:841730]\nrecreation =  dataset[850000:852000]\nreference =  dataset[955250:957250]\nscience =  dataset[1013000:1015000]\nshopping =  dataset[1143000:1145000]\nsociety =  dataset[1293000:1295000]\nsports =  dataset[1492000:1494000]\n\ntest_data = pd.concat([adult, arts, business, computers, games, health, home, \n              kids, news, recreation, reference,science,shopping, society, sports], axis=0)\n\ndataset.drop(dataset.index[1:2000],inplace= True)\ndataset.drop(dataset.index[50000:52000],inplace= True)\ndataset.drop(dataset.index[520000:522000],inplace= True)\ndataset.drop(dataset.index[535300:537300],inplace= True)\ndataset.drop(dataset.index[650000:652000],inplace= True)\ndataset.drop(dataset.index[710000:712000],inplace= True)\ndataset.drop(dataset.index[764200:766200],inplace= True)\ndataset.drop(dataset.index[793080:795080],inplace= True)\ndataset.drop(dataset.index[839730:841730],inplace= True)\ndataset.drop(dataset.index[850000:852000],inplace= True)\ndataset.drop(dataset.index[955250:957250],inplace= True)\ndataset.drop(dataset.index[1013000:1015000],inplace= True)\ndataset.drop(dataset.index[1143000:1145000],inplace= True)\ndataset.drop(dataset.index[1293000:1295000],inplace= True)\ndataset.drop(dataset.index[1492000:1494000],inplace= True)\ndataset.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"aWZZudRJm1_Y","outputId":"90e2856e-0ceb-4feb-8a55-abfabefd51d5"},"cell_type":"code","source":"import seaborn as sns\nax = sns.countplot(y=\"Category\",  data=df )\nplt.title(\"Visualization of the dataset\", y=1.01, fontsize=20)\nplt.ylabel(\"Name of the Category\", labelpad=15)\nplt.xlabel(\"Number of Categories of URLs\", labelpad=15)\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"_4fjWR0-m1_i","outputId":"10527435-4eaa-42c7-d554-d60ed9704ec8"},"cell_type":"code","source":"ax = sns.countplot(y = \"Category\",  data = dataset )\nplt.title(\"Visualization of the train dataset\", y=1.01, fontsize=20)\nplt.ylabel(\"Name of the Category\", labelpad=15)\nplt.xlabel(\"Number of Categories of URLs\", labelpad=15)\ndataset.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"9NzUcQqtm1_m","outputId":"d35c0756-bad0-4ca0-9cc8-bc5676f41b1d"},"cell_type":"code","source":"ax = sns.countplot(y = \"Category\",  data = test_data , color = 'gray')\nplt.title(\"Visualization of the test dataset\", y=1.01, fontsize=20)\nplt.ylabel(\"Name of the Category\", labelpad=15)\nplt.xlabel(\"Number of Categories of URLs\", labelpad=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"wqRmxpeRm1_p"},"cell_type":"code","source":"dataset = pd.get_dummies(dataset  ,prefix='Category', columns = ['Category'])\ntest_data = pd.get_dummies(test_data  ,prefix='Category', columns = ['Category'])\n#print('df', df[:2])\nbackup_df = df\n#print('df1', backup_df[:2])\n#print('df2', df2[:2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"yyMy4_9Jm1_x","outputId":"5029d239-45cd-466a-f99e-646f7b1a3003"},"cell_type":"code","source":"test_data[:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"SAP9hwxOm1_1","outputId":"67010249-a2cc-47f0-91fd-2277643acf6e"},"cell_type":"code","source":"X_train=dataset['URL']\ny_train=dataset.iloc[: , 1:16].values\nprint(y_train[:3])\nprint( 'y_train shape' , y_train.shape)\n\nX_test=test_data['URL']\ny_test=test_data.iloc[: , 1:16].values\nprint( 'y_test shape' , y_test.shape)\nX_train[:3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"rsQYNnVnm1_6","outputId":"024decc5-59ef-42ea-d3a6-109c5e3d8c4a"},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\ndef create_and_train_tokenizer(texts):\n    tokenizer=Tokenizer()\n    tokenizer.fit_on_texts(texts)\n    return tokenizer\n\nfrom keras.preprocessing.sequence import pad_sequences\ndef encode_reviews(tokenizer, max_length, docs):\n    encoded=tokenizer.texts_to_sequences(docs) \n    padded=pad_sequences(encoded, maxlen=max_length, padding=\"post\")\n    return padded\n\ntokenizer=create_and_train_tokenizer(texts = X_train)\nvocab_size=len(tokenizer.word_index) + 1\nprint(\"Vocabulary size:\", vocab_size)\n\nmax_length=max([len(row.split()) for row in X_train])\nprint(\"Maximum length:\",max_length)\n\nX_train_encoded = encode_reviews(tokenizer, max_length, X_train)\nX_test_encoded = encode_reviews(tokenizer, max_length, X_test)\nprint('x_train shape:', X_train_encoded.shape)\nprint('x_test shape:', X_test_encoded.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"2lot5FYVm2AA"},"cell_type":"code","source":"from keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.layers import Embedding\nfrom keras.layers import Conv1D, GlobalMaxPooling1D\nfrom keras.datasets import imdb\nfrom keras import layers, models\nfrom keras.callbacks import EarlyStopping","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"lsCCkNt_m2AE","outputId":"9031a276-7d87-4f50-de17-cc0a392961e5"},"cell_type":"code","source":"X_train_encoded[:2]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"id":"6r-8erJlm2AH","outputId":"8620dd79-a746-439a-eb60-c9ee4e1a6059"},"cell_type":"code","source":"from keras import layers, models\n\ndef create_embedding_model(vocab_size, max_length):\n    model=models.Sequential()\n    model.add(layers.Embedding(vocab_size, 100, input_length=max_length))\n\n    model.add(layers.Conv1D(256, 1 , activation=\"relu\"))\n    #model.add(layers.BatchNormalization())\n    model.add(layers.MaxPooling1D())\n    \n    model.add(layers.Conv1D(256, 1, activation=\"relu\"))\n    #model.add(layers.BatchNormalization())\n    model.add(layers.MaxPooling1D())\n\n    model.add(layers.Flatten())\n    model.add(layers.Dense(128,  activation=\"relu\"))\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(15,  activation=\"softmax\"))   \n    return model\n    \n\nembedding_model = create_embedding_model(vocab_size=vocab_size , max_length=max_length)\nembedding_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"GGapdMZ4m2AK","outputId":"ac0c7144-2521-4b34-d9f1-23a338d9c97d"},"cell_type":"code","source":"print(len(y_train) + len(y_test))\nprint(len(X_train_encoded))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"1e2kN2wcm2AP","outputId":"680e44f6-32de-4abc-a4d3-dbca02b8a804"},"cell_type":"code","source":"from keras.optimizers import SGD\nopt = SGD(lr=0.01, momentum=0.9)\nembedding_model.compile(loss='categorical_crossentropy',\n              optimizer= opt,\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"WL9nlq64m2AT","outputId":"c949ba04-aff8-4164-e626-e2a4a18cb8f2"},"cell_type":"code","source":"#earlyStopping = EarlyStopping(monitor=\"val_accuracy\", patience=1)\nmodelHistory = embedding_model.fit(X_train_encoded, \n                                   y_train, \n                                   validation_data=(X_test_encoded, y_test),\n                                   epochs= 38                                  )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"Ll5fsdAym2Ak"},"cell_type":"code","source":"import tensorflow as tf\n# load and evaluate a saved model\nfrom numpy import loadtxt\nfrom keras.models import load_model\nimport tensorflow as tf\n\n#saving the model\nembedding_model.save('new_classification_model_stem(opt(3,3)).h5')\n# load model\n#embedding_model = load_model('new_classification_model_stem(opt(3,3)).h5')\n# summarize model.\nembedding_model.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"ovFj8qIpm2AX"},"cell_type":"code","source":"_, acc = embedding_model.evaluate(X_train_encoded, y_train, verbose=0)\nprint(\"Train accuracy:{:.2f}\".format(acc*100))\n_,acc= embedding_model.evaluate(X_test_encoded, y_test, verbose=0)\nprint(\"Test accuracy:{:.2f}\".format(acc*100))","execution_count":null,"outputs":[]},{"metadata":{"id":"ggJEAndHR2b5","trusted":true},"cell_type":"code","source":"# show a nicely formatted classification report\ny_pred = (embedding_model.predict(X_test_encoded))\ny_pred = (y_pred > 0.5) \n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred, digits = 4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"BT46oXd-m2Aa"},"cell_type":"code","source":"from keras.callbacks import History \nhistory = History()\nloss_train = embedding_model.history.history['loss']\nloss_val = embedding_model.history.history['val_loss']\nepochs = range(1,39)\nplt.plot(epochs, loss_train, 'g', label='Training loss')\nplt.plot(epochs, loss_val, 'b', label='validation loss')\nplt.title('Training and Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend(),\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"EVqNiW16m2Ah"},"cell_type":"code","source":"loss_train = embedding_model.history.history['accuracy']\nloss_val = embedding_model.history.history['val_accuracy']\nepochs = range(1,39)\nplt.plot(epochs, loss_train, 'g', label='Training accuracy')\nplt.plot(epochs, loss_val, 'b', label='validation accuracy')\nplt.title('Training and Validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"TuulVmMqm2Ao"},"cell_type":"code","source":"from nltk.tokenize import word_tokenize \nurl = ['http://www.bdnews24.com', 'http://www.bdnews24.com',  'http://www.yuzutree.com', 'http://www.fcbarcelona.com']\n#,'http://www.bdnews24.com', 'http://www.yuzutree.com'\nurl1 = perform_stemming(websites = url)\noutput = list()\nword1 = list()\ndef processed_urls(website):\n    for url in website:\n        if len(url)>20:\n            url_data = url[:20]\n        else:\n            url_data = url\n            \n        #word1 = ' '.join(url_data)\n        output.append(url_data)\n        \n    return output\n  \nprocessed_url = processed_urls(url1)\npadded = encode_reviews(tokenizer, 20 ,processed_url)\n\ncategories = ['adult', 'arts', 'business', 'computers', 'games', 'health', 'home', \n              'kids', 'news', 'recreation', 'reference','science','shopping', 'society', 'sports']\n\ny_pred1= embedding_model.predict(padded)\ncount  = 0\ncategory = 0\nfor numbers in y_pred1:\n  max_number = 1e-10\n  count = 0;\n  for number in numbers:\n    count+=1\n    if number > max_number:\n      max_number = number;\n      category = count\n  print('category is :', categories[category-1:category])\n\nprint(category)\n#y_pred\n#from sklearn.metrics import classification_report\n#print(classification_report(y_test, y_pred, digits = 4))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"final Copy of website_using_CNN((1,1)).ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":4}