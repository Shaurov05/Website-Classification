{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn import linear_model\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\n%matplotlib inline \nfrom sklearn.utils import shuffle\nimport matplotlib.pyplot as plt\nfrom numpy import array\nfrom numpy import argmax\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_selection import chi2\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Activation\nfrom keras.utils import np_utils\nimport re\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import one_hot\nfrom keras.preprocessing.text import text_to_word_sequence\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import SGDClassifier","execution_count":1,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"names=['URL','Category']\n#df=pd.read_csv( \"../input/website-classification-using-url/URL Classification.csv\")\n#df=pd.read_csv('../input/Website classification using URL/URL Classification.csv')\ndf=pd.read_csv('../input/website-classification-using-url/URL Classification.csv',names=names, na_filter=False)","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlb = LabelEncoder()\nlb.fit(df['Category'])\ndf['Category'] = lb.transform(df['Category'])\n\ndata = pd.get_dummies(df,prefix=['Category'], columns = ['Category'])\ndf = data\ndf[:2]","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"                                URL  Category_0  Category_1  Category_2  \\\n1  http://www.liquidgeneration.com/           1           0           0   \n2       http://www.onlineanime.org/           1           0           0   \n\n   Category_3  Category_4  Category_5  Category_6  Category_7  Category_8  \\\n1           0           0           0           0           0           0   \n2           0           0           0           0           0           0   \n\n   Category_9  Category_10  Category_11  Category_12  Category_13  Category_14  \n1           0            0            0            0            0            0  \n2           0            0            0            0            0            0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>URL</th>\n      <th>Category_0</th>\n      <th>Category_1</th>\n      <th>Category_2</th>\n      <th>Category_3</th>\n      <th>Category_4</th>\n      <th>Category_5</th>\n      <th>Category_6</th>\n      <th>Category_7</th>\n      <th>Category_8</th>\n      <th>Category_9</th>\n      <th>Category_10</th>\n      <th>Category_11</th>\n      <th>Category_12</th>\n      <th>Category_13</th>\n      <th>Category_14</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>http://www.liquidgeneration.com/</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>http://www.onlineanime.org/</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = df[1:4000]\ndf2 = df[50000:54000]\ndf3 = df[520000:524000]\ndf4 =df[535300:539300]\ndf5 = df[650000:654000]\ndf6= df[710000:714000]\ndf7=  df[764200:768200]\ndf8=  df[793080:797080]\ndf9=  df[839730:843730]\ndf10=  df[850000:854000]\ndf11=  df[955250:959250]\ndf12=  df[1013000:1017000]\ndf13=  df[1143000:1147000]\ndf14=  df[1293000:1297000]\ndf15=  df[1492000:1496000]\n#df6 = df[77000:1562978]\ndt=pd.concat([df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13,df14,df15], axis=0)\ndf.drop(df.index[1:4000],inplace= True)\ndf.drop(df.index[50000:54000],inplace= True)\ndf.drop(df.index[520000:524000],inplace= True)\ndf.drop(df.index[535300:539300],inplace= True)\ndf.drop(df.index[650000:654000],inplace= True)\ndf.drop(df.index[710000:714000],inplace= True)\ndf.drop(df.index[764200:768200],inplace= True)\ndf.drop(df.index[793080:797080],inplace= True)\ndf.drop(df.index[839730:843730],inplace= True)\ndf.drop(df.index[850000:854000],inplace= True)\ndf.drop(df.index[955250:959250],inplace= True)\ndf.drop(df.index[1013000:1017000],inplace= True)\ndf.drop(df.index[1143000:1147000],inplace= True)\ndf.drop(df.index[1293000:1297000],inplace= True)\ndf.drop(df.index[1492000:1496000],inplace= True)\ndf.tail()","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"                                                     URL  Category_0  \\\n1562974                         http://www.maxpreps.com/           0   \n1562975                          http://www.myscore.com/           0   \n1562976      http://sportsillustrated.cnn.com/highschool           0   \n1562977  http://rss.cnn.com/rss/si_highschool?format=xml           0   \n1562978            http://www.usatoday.com/sports/preps/           0   \n\n         Category_1  Category_2  Category_3  Category_4  Category_5  \\\n1562974           0           0           0           0           0   \n1562975           0           0           0           0           0   \n1562976           0           0           0           0           0   \n1562977           0           0           0           0           0   \n1562978           0           0           0           0           0   \n\n         Category_6  Category_7  Category_8  Category_9  Category_10  \\\n1562974           0           0           0           0            0   \n1562975           0           0           0           0            0   \n1562976           0           0           0           0            0   \n1562977           0           0           0           0            0   \n1562978           0           0           0           0            0   \n\n         Category_11  Category_12  Category_13  Category_14  \n1562974            0            0            0            1  \n1562975            0            0            0            1  \n1562976            0            0            0            1  \n1562977            0            0            0            1  \n1562978            0            0            0            1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>URL</th>\n      <th>Category_0</th>\n      <th>Category_1</th>\n      <th>Category_2</th>\n      <th>Category_3</th>\n      <th>Category_4</th>\n      <th>Category_5</th>\n      <th>Category_6</th>\n      <th>Category_7</th>\n      <th>Category_8</th>\n      <th>Category_9</th>\n      <th>Category_10</th>\n      <th>Category_11</th>\n      <th>Category_12</th>\n      <th>Category_13</th>\n      <th>Category_14</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1562974</th>\n      <td>http://www.maxpreps.com/</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1562975</th>\n      <td>http://www.myscore.com/</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1562976</th>\n      <td>http://sportsillustrated.cnn.com/highschool</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1562977</th>\n      <td>http://rss.cnn.com/rss/si_highschool?format=xml</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1562978</th>\n      <td>http://www.usatoday.com/sports/preps/</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train=df['URL']\ny_train=df.iloc[: , 1:16].values\nprint(y_train)\ny_train.shape","execution_count":5,"outputs":[{"output_type":"stream","text":"[[1 0 0 ... 0 0 0]\n [1 0 0 ... 0 0 0]\n [1 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 1]\n [0 0 0 ... 0 0 1]\n [0 0 0 ... 0 0 1]]\n","name":"stdout"},{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"(1502979, 15)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test=dt['URL']\ny_test=dt.iloc[: , 1:16].values\nprint(y_test)\ny_test.shape","execution_count":6,"outputs":[{"output_type":"stream","text":"[[1 0 0 ... 0 0 0]\n [1 0 0 ... 0 0 0]\n [1 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 1]\n [0 0 0 ... 0 0 1]\n [0 0 0 ... 0 0 1]]\n","name":"stdout"},{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"(59999, 15)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"import re\nimport nltk\nfrom sklearn.pipeline import Pipeline\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nstemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\nclass StemmedCountVectorizer(CountVectorizer):\n    def build_analyzer(self):\n        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n    \nstemmed_count_vect = StemmedCountVectorizer(stop_words='english', ngram_range=(2,2))\n\ngs_clf = Pipeline([('vect', stemmed_count_vect),\n                   ('tfidf', TfidfTransformer()),\n                   ('clf', SGDClassifier(loss='perceptron', penalty='l2',\n                    alpha =1e-4 , max_iter=20 ,tol=None)),\n   ])\ngs_clf = gs_clf.fit(X_train, y_train)\n"},{"metadata":{},"cell_type":"markdown","source":"**This is for understandind the basics of stemming it transforms the words to its root**\n* >  stem_vectorizer = stemmed_count_vect\n* > text1 = 'ï am so much bored because of your meaningless behaviour'\n* > print(stem_vectorizer.fit_transform([text1]))\n* > print(stem_vectorizer.get_feature_names())"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\ndef create_and_train_tokenizer(texts):\n    tokenizer=Tokenizer()\n    tokenizer.fit_on_texts(texts)\n    return tokenizer\n\nfrom keras.preprocessing.sequence import pad_sequences\ndef encode_reviews(tokenizer, max_length, docs):\n    encoded=tokenizer.texts_to_sequences(docs) \n    padded=pad_sequences(encoded, maxlen=max_length, padding=\"post\")\n    return padded\n\ntokenizer=create_and_train_tokenizer(texts = X_train)\nvocab_size=len(tokenizer.word_index) + 1\nprint(\"Vocabulary size:\", vocab_size)\n\nmax_length=max([len(row.split()) for row in X_train])\nprint(\"Maximum length:\",max_length)\n\nX_train_encoded = encode_reviews(tokenizer, max_length, X_train)\nX_test_encoded = encode_reviews(tokenizer, max_length, X_test)\nprint('x_train shape:', X_train_encoded.shape)\nprint('x_test shape:', X_test_encoded.shape)","execution_count":7,"outputs":[{"output_type":"stream","text":"Vocabulary size: 1182241\nMaximum length: 16\nx_train shape: (1502979, 16)\nx_test shape: (59999, 16)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.layers import Embedding\nfrom keras.layers import Conv1D, GlobalMaxPooling1D\nfrom keras.datasets import imdb\nfrom keras import layers, models\nfrom keras.callbacks import EarlyStopping","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_encoded[:2]","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"array([[     1,      2, 201614,      3,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0],\n       [     1,      2,   3662,   3193,      3,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0]],\n      dtype=int32)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import layers, models\n\ndef create_embedding_model(vocab_size, max_length):\n    model=models.Sequential()\n    model.add(layers.Embedding(vocab_size, 100, input_length=max_length))\n\n    model.add(layers.Conv1D(1024, 5, activation=\"relu\"))\n    #model.add(layers.BatchNormalization())\n    model.add(layers.MaxPooling1D())\n\n    model.add(layers.Conv1D(1024, 5, activation=\"relu\"))\n    #model.add(layers.BatchNormalization())\n    model.add(layers.MaxPooling1D())\n\n    model.add(layers.Flatten())\n    model.add(layers.Dense(512,  activation=\"relu\")) \n    dropout = Dropout(0.5)\n    model.add(layers.Dense(15,  activation=\"softmax\"))   \n    return model\n\nembedding_model = create_embedding_model(vocab_size=vocab_size, max_length=max_length)\nembedding_model.summary()\n\nfrom keras.optimizers import SGD\n#opt = SGD(lr=0.01, momentum=0.9)\nembedding_model.compile(loss='categorical_crossentropy',\n              optimizer= 'adam',\n              metrics=['accuracy'])","execution_count":10,"outputs":[{"output_type":"stream","text":"Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, 16, 100)           118224100 \n_________________________________________________________________\nconv1d_1 (Conv1D)            (None, 12, 1024)          513024    \n_________________________________________________________________\nmax_pooling1d_1 (MaxPooling1 (None, 6, 1024)           0         \n_________________________________________________________________\nconv1d_2 (Conv1D)            (None, 2, 1024)           5243904   \n_________________________________________________________________\nmax_pooling1d_2 (MaxPooling1 (None, 1, 1024)           0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 1024)              0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 512)               524800    \n_________________________________________________________________\ndense_2 (Dense)              (None, 15)                7695      \n=================================================================\nTotal params: 124,513,523\nTrainable params: 124,513,523\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#earlyStopping = EarlyStopping(monitor=\"val_accuracy\", patience=1)\nmodelHistory = embedding_model.fit(X_train_encoded, \n                                   y_train, \n                                   validation_data=(X_test_encoded, y_test),\n                                   epochs= 15\n                                   )","execution_count":11,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","name":"stderr"},{"output_type":"stream","text":"Train on 1502979 samples, validate on 59999 samples\nEpoch 1/15\n1502979/1502979 [==============================] - 2061s 1ms/step - loss: 1.6712 - accuracy: 0.4813 - val_loss: 1.9090 - val_accuracy: 0.4441\nEpoch 2/15\n1502979/1502979 [==============================] - 2054s 1ms/step - loss: 0.8913 - accuracy: 0.7315 - val_loss: 1.0954 - val_accuracy: 0.7217\nEpoch 3/15\n1502979/1502979 [==============================] - 2053s 1ms/step - loss: 0.3888 - accuracy: 0.8884 - val_loss: 0.7224 - val_accuracy: 0.8035\nEpoch 4/15\n1502979/1502979 [==============================] - 2064s 1ms/step - loss: 0.2848 - accuracy: 0.9174 - val_loss: 0.6280 - val_accuracy: 0.8308\nEpoch 5/15\n1502979/1502979 [==============================] - 2065s 1ms/step - loss: 0.2385 - accuracy: 0.9283 - val_loss: 0.6470 - val_accuracy: 0.8441\nEpoch 6/15\n1502979/1502979 [==============================] - 2054s 1ms/step - loss: 0.2149 - accuracy: 0.9349 - val_loss: 0.6253 - val_accuracy: 0.8485\nEpoch 7/15\n1502979/1502979 [==============================] - 2056s 1ms/step - loss: 0.1970 - accuracy: 0.9395 - val_loss: 0.6641 - val_accuracy: 0.8478\nEpoch 8/15\n1502979/1502979 [==============================] - 2058s 1ms/step - loss: 0.1901 - accuracy: 0.9435 - val_loss: 0.6422 - val_accuracy: 0.8436\nEpoch 9/15\n1502979/1502979 [==============================] - 2061s 1ms/step - loss: 0.1713 - accuracy: 0.9462 - val_loss: 0.6502 - val_accuracy: 0.8523\nEpoch 10/15\n1502979/1502979 [==============================] - 2053s 1ms/step - loss: 0.1631 - accuracy: 0.9486 - val_loss: 0.6339 - val_accuracy: 0.8553\nEpoch 11/15\n1502979/1502979 [==============================] - 2061s 1ms/step - loss: 0.1532 - accuracy: 0.9510 - val_loss: 0.6738 - val_accuracy: 0.8812\nEpoch 12/15\n1502979/1502979 [==============================] - 2053s 1ms/step - loss: 0.1499 - accuracy: 0.9527 - val_loss: 0.6134 - val_accuracy: 0.8819\nEpoch 13/15\n1502979/1502979 [==============================] - 2052s 1ms/step - loss: 0.1392 - accuracy: 0.9547 - val_loss: 0.8730 - val_accuracy: 0.8542\nEpoch 14/15\n1502979/1502979 [==============================] - 2053s 1ms/step - loss: 0.1358 - accuracy: 0.9558 - val_loss: 0.6433 - val_accuracy: 0.8837\nEpoch 15/15\n1502979/1502979 [==============================] - 2052s 1ms/step - loss: 0.1343 - accuracy: 0.9571 - val_loss: 0.7545 - val_accuracy: 0.8566\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(y_train) + len(y_test))\nprint(len(X_train_encoded))","execution_count":12,"outputs":[{"output_type":"stream","text":"1562978\n1502979\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"_, acc = embedding_model.evaluate(X_train_encoded, y_train, verbose=0)\nprint(\"Train accuracy:{:.2f}\".format(acc*100))\n_,acc= embedding_model.evaluate(X_test_encoded, y_test, verbose=0)\nprint(\"Test accuracy:{:.2f}\".format(acc*100))","execution_count":13,"outputs":[{"output_type":"stream","text":"Train accuracy:96.47\nTest accuracy:85.66\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"print(gs_clf.predict(['http://www.banglainfotube.com']))\nprint(gs_clf.predict(['http://www.gamespot.net/']))"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\n#saving the model\nembedding_model.save('classification_model.h5')\n# Recreate the exact same model, including its weights and the optimizer\nnew_model = tf.keras.models.load_model('classification_model.h5')\n# Show the model architecture\nnew_model.summary()\n# Evaluate the model\nloss, acc = new_model.evaluate(X_test_encoded, y_test, verbose=0)\nprint('Restored model, accuracy 1: {:5.2f}%'.format(100*acc))\n\n''''''''''\n# Save the weights\nmodel.save_weights('./checkpoints/my_checkpoint')\n# Create a new model instance\nmodel = create_model()\n# Restore the weights\nmodel.load_weights('./checkpoints/my_checkpoint')\n# Evaluate the model\nloss,acc = model.evaluate(X_test_encoded, y_test, verbose=0)\nprint(\"Restored model, accuracy 2: {:5.2f}%\".format(100*acc))''''\n\n","execution_count":19,"outputs":[{"output_type":"stream","text":"Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, 16, 100)           118224100 \n_________________________________________________________________\nconv1d_1 (Conv1D)            (None, 12, 1024)          513024    \n_________________________________________________________________\nmax_pooling1d_1 (MaxPooling1 (None, 6, 1024)           0         \n_________________________________________________________________\nconv1d_2 (Conv1D)            (None, 2, 1024)           5243904   \n_________________________________________________________________\nmax_pooling1d_2 (MaxPooling1 (None, 1, 1024)           0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 1024)              0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 512)               524800    \n_________________________________________________________________\ndense_2 (Dense)              (None, 15)                7695      \n=================================================================\nTotal params: 124,513,523\nTrainable params: 124,513,523\nNon-trainable params: 0\n_________________________________________________________________\nRestored model, accuracy 1: 85.66%\n","name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"name 'model' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-11dcd28dcd16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Save the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./checkpoints/my_checkpoint'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m# Create a new model instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_maxlength = 0\nfor text in X_test:\n    length = len(text) \n    if test_maxlength < length:\n        test_maxlength = length\n        ntext = text\n        \nprint('maxlength is {} and text is = {} '.format(test_maxlength , ntext))","execution_count":15,"outputs":[{"output_type":"stream","text":"maxlength is 312 and text is = http://www.ams.usda.gov/amsv1.0/ams.fetchtemplatedata.do?template=templaten&amp;navid=soybeanprogrambackgroundinformation&amp;rightnav1=soybeanprogrambackgroundinformation&amp;topnav=&amp;leftnav=industrymarketingandpromotion&amp;page=soybeanbackgroundofsoybeancheckoffprogram&amp;resulttype=&amp;acct=lspromores \n","name":"stdout"}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}